{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudoYan/Team-3/blob/master/CompVisPi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOmLB6I2Q95W",
        "colab_type": "code",
        "outputId": "4214408d-3f20-4124-fa38-72a588b3bf24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import torchvision\n",
        "from collections import OrderedDict\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "from os.path import exists\n",
        "data_dir = '/home/pi/PlantVillage'\n",
        "train_dir = data_dir + '/train'\n",
        "valid_dir = data_dir + '/val'\n",
        "nThreads = 4\n",
        "batch_size = 32\n",
        "use_gpu = torch.cuda.is_available()\n",
        "import json\n",
        "\n",
        "with open(r'/home/pi/Desktop/categories.json') as f:\n",
        "    cat_to_name = json.load(f)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        #transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Load the datasets with ImageFolder\n",
        "\n",
        "data_dir = '/home/pi/PlantVillage'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "\n",
        "# Using the image datasets and the trainforms, define the dataloaders\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "class_names = image_datasets['train'].classes\n",
        "model = models.resnet152(pretrained=True)\n",
        "# Freeze parameters so we don't backprop through them\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#Let's check the model architecture:\n",
        "    print(model)\n",
        "from collections import OrderedDict\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(2048, 512)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('fc2', nn.Linear(512, 39)),\n",
        "                          ('output', nn.LogSoftmax(dim=1))\n",
        "                          ]))\n",
        "model.fc = classifier\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=20):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best valid accuracy: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "num_epochs = 10\n",
        "if use_gpu:\n",
        "    print (\"Using GPU: \"+ str(use_gpu))\n",
        "    model = model.cuda()\n",
        "\n",
        "# NLLLoss because our output is LogSoftmax\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Adam optimizer with a learning rate\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "# Decay LR by a factor of 0.1 every 5 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "\n",
        "model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10)\n",
        "def test(model, dataloaders, device):\n",
        "  model.eval()\n",
        "  accuracy = 0\n",
        "  \n",
        "  model.to(device)\n",
        "    \n",
        "  for images, labels in dataloaders['val']:\n",
        "    images = Variable(images)\n",
        "    labels = Variable(labels)\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "      \n",
        "    output = model.forward(images)\n",
        "    ps = torch.exp(output)\n",
        "    equality = (labels.data == ps.max(1)[1])\n",
        "    accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
        "      \n",
        "    print(\"Testing Accuracy: {:.3f}\".format(accuracy/len(dataloaders['val'])))\n",
        "test(model, dataloaders, device)\n",
        "model.class_to_idx = dataloaders['train'].dataset.class_to_idx\n",
        "model.epochs = num_epochs\n",
        "checkpoint = {'input_size': [3, 224, 224],\n",
        "                 'batch_size': dataloaders['train'].batch_size,\n",
        "                  'output_size': 39,\n",
        "                  'state_dict': model.state_dict(),\n",
        "                  'data_transforms': data_transforms,\n",
        "                  'optimizer_dict':optimizer.state_dict(),\n",
        "                  'class_to_idx': model.class_to_idx,\n",
        "                  'epoch': model.epochs}\n",
        "torch.save(checkpoint, '/home/pi/plants9615_checkpoint.pth')\n",
        "def process_image(image):\n",
        "    # Process a PIL image for use in a PyTorch model\n",
        "\n",
        "    size = 256, 256\n",
        "    image.thumbnail(size, Image.ANTIALIAS)\n",
        "    image = image.crop((128 - 112, 128 - 112, 128 + 112, 128 + 112))\n",
        "    npImage = np.array(image)\n",
        "    npImage = npImage/255.\n",
        "        \n",
        "    imgA = npImage[:,:,0]\n",
        "    imgB = npImage[:,:,1]\n",
        "    imgC = npImage[:,:,2]\n",
        "    \n",
        "    imgA = (imgA - 0.485)/(0.229) \n",
        "    imgB = (imgB - 0.456)/(0.224)\n",
        "    imgC = (imgC - 0.406)/(0.225)\n",
        "        \n",
        "    npImage[:,:,0] = imgA\n",
        "    npImage[:,:,1] = imgB\n",
        "    npImage[:,:,2] = imgC\n",
        "    \n",
        "    npImage = np.transpose(npImage, (2,0,1))\n",
        "    \n",
        "    return npImage\n",
        "def imshow(image, ax=None, title=None):\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    \n",
        "    # PyTorch tensors assume the color channel is the first dimension\n",
        "    # but matplotlib assumes is the third dimension\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    # Undo preprocessing\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    \n",
        "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
        "    image = np.clip(image, 0, 1)\n",
        "    \n",
        "    ax.imshow(image)\n",
        "    \n",
        "    return ax\n",
        "def predict(image_path, model, topk=5):\n",
        "    \n",
        "    # Implement the code to predict the class from an image file\n",
        "    \n",
        "    image = torch.FloatTensor([process_image(Image.open(image_path))])\n",
        "    model.eval()\n",
        "    output = model.forward(Variable(image))\n",
        "    pobabilities = torch.exp(output).data.numpy()[0]\n",
        "    \n",
        "\n",
        "    top_idx = np.argsort(pobabilities)[-topk:][::-1] \n",
        "    top_class = [idx_to_class[x] for x in top_idx]\n",
        "    top_probability = pobabilities[top_idx]\n",
        "\n",
        "    return top_probability, top_class\n",
        "print (predict('/home/pi/PlantVillage/val/Blueberry___healthy/06eacfab-fb39-40e0-bbce-927bc98fa2ac___RS_HL 2663.JPG', loaded_model))\n",
        "def view_classify(img, probabilities, classes, mapper):\n",
        "    img_filename = img.split('/')[-2]\n",
        "    img = Image.open(img)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,10), ncols=1, nrows=2)\n",
        "    flower_name = mapper[img_filename]\n",
        "    \n",
        "    ax1.set_title(flower_name)\n",
        "    ax1.imshow(img)\n",
        "    ax1.axis('off')\n",
        "    \n",
        "    y_pos = np.arange(len(probabilities))\n",
        "    ax2.barh(y_pos, probabilities)\n",
        "    ax2.set_yticks(y_pos)\n",
        "    ax2.set_yticklabels([mapper[x] for x in classes])\n",
        "    ax2.invert_yaxis()\n",
        "img = '/home/pi/PlantVillage/val/Corn_(maize)___Northern_Leaf_Blight/00a14441-7a62-4034-bc40-b196aeab2785___RS_NLB 3932.JPG'\n",
        "p, c = predict(img, loaded_model)\n",
        "view_classify(img, p, c, cat_to_name)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1df07b0d10e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/home/pi/Desktop/categories.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mcat_to_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m data_transforms = {\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/pi/Desktop/categories.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlmOv976TdAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = models.resnet152()\n",
        "    \n",
        "    # Our input_size matches the in_features of pretrained model\n",
        "    input_size = 2048\n",
        "    output_size = 39\n",
        "    classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(2048, 512)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          #('dropout1', nn.Dropout(p=0.2)),\n",
        "                          ('fc2', nn.Linear(512, 39)),\n",
        "                          ('output', nn.LogSoftmax(dim=1))\n",
        "                          ]))\n",
        "\n",
        "# Replacing the pretrained model classifier with our classifier\n",
        "    model.fc = classifier\n",
        "    \n",
        "    \n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    \n",
        "    return model, checkpoint['class_to_idx']\n",
        "\n",
        "# Get index to class mapping\n",
        "loaded_model, class_to_idx = load_checkpoint('plants9615_checkpoint.pth')\n",
        "idx_to_class = { v : k for k,v in class_to_idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}